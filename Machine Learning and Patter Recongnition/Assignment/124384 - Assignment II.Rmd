---
title: "Machine Learning and Patter Recognition Assignment II"
author: "124384 - Luycer Bosire"
date: "7/20/2021"
output:
  pdf_document: default
  html_document: default
---

#Question 1: Build classification model using logistic regression, discriminant analysis and classification trees from the *heart_data* using the package caret and tree in R.
# Introduction

Our data analysis runs several classification models and prediction models on two data sets; `heart_all.csv` and the `Boston` data set. We compare the performance of both models and select the model that best works based on metrics such as accuracy, sensitivity, specificity, PPV, NPV for classification queries and the r-squared value for prediction queries.

The data sets are as follows:

*Heart_all.csv*

The dataset contains 303 records on patients and 14 features that could be a risk factor of disease. The classification goal is to predict whether the patient has AHD. 

*Boston data*

The dataset (in library `MASS`) contains 506 records and 14 features on house prices. The prediction goal is the house price.


## Part a
In this first part, we fit our models through logistic regression models, linear discriminant analysis and classification trees. After fitting our models, we chaeck for our classification accuracy through the various metrics as mentioned in the introduction.

### Section 1: Fit models

We load all the libraries required to perform the computation in this assignment.
install.packages("tree", "Hmisc")

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(tree)
library(rpart)
library(rpart.plot)
library(kableExtra)
library(ROCR)
library(MASS)
```

We then read in the data and conduct pre-processing in preparation for model fitting.
```{r}
heart <- read.csv("heart_all.csv")
# Mutate character features to factors/categorical variables.
heart <- heart %>% mutate_if(is.character, as.factor)
head(heart)
```
We then split the data into data partitioning in a 80% (test data) and 20% (training data) ratio.

```{r}
# Omit any missing data
heart2 <- na.omit(heart)

set.seed(1234)
# create partition
samples_test <- heart2$AHD %>% 
  createDataPartition(p = 0.8, list = FALSE)
# Assign train and test
train.data  <-heart2[samples_test, ]
test.data <- heart2[-samples_test, ]
```

We then fit the models:

1. Logistic regression

```{r}
log_model <- glm( AHD~., data = train.data, family = binomial)

# Summarize the model
summary(log_model)
```
Based on the logistic model, sex, chest pain, slope and Ca: Number of major vessels colored by flourosopy are significant predictors of heart disease.

We make our predictions:

```{r}
# Make predictions
predicted_log <- log_model %>% 
  predict(test.data, type = "response")
predicted.log <- ifelse(predicted_log > 0.5, "Yes", "No")
```

```{r}
# Create logistic regression confusion matrix
table_log <- table(predicted.log, test.data$AHD)
```

We then take a look at model accuracy
```{r}
# Model accuracy 
mean(predicted.log == test.data$AHD)
```
The model accuracy is **86.4%**


2. Linear Discriminant Analysis

We then fit a linear discriminant model on the data.

```{r}
library(MASS)
# fit the LDA model
heart.lda <- lda(AHD ~ ., data = train.data)
heart.lda
```


We then estimate the pre-processing parameters before transforming it.
```{r}
prep_p <- train.data %>% 
  preProcess(method = c("center", "scale"))
```


```{r}
# Transform the data using the estimated parameters
trans.train <- prep_p %>% 
  predict(train.data)
trans.test <- prep_p %>% 
  predict(test.data)
```

We the fit the model using the transformed data:
```{r}
# Model transformed
heart2.lda <- lda(AHD ~ ., data = trans.train)

```

```{r}
# Make predictions
predictions.lda <- heart2.lda  %>% predict(trans.test)

```

```{r}
# Create confusion matrix for LDA
table_lda <- table(predictions.lda$class,test.data$AHD)
table_lda
```
We then check the model accuracy
```{r}
mean(predictions.lda$class == trans.test$AHD)
```
The model accuracy is **88.1%**

3. Classification tree

```{r, fig.height=8, fig.width=8, fig.cap="Classification tree of heart disease"}
# fit the decision tree using rpart
fit_class_tree <- rpart(AHD~., data = train.data, method = 'class')
rpart.plot(fit_class_tree)
```

We then make predictions using the decision tree.
```{r}
# predicted classes
predict_test <-predict(fit_class_tree, test.data, type = 'class')
# predicted probabilities
pred_prob <-predict(fit_class_tree, test.data, type = 'prob')
```

We then take a look at the confusion matrix.
```{r}
table_class_tree <- table(test.data$AHD, predict_test)
table_class_tree
```

```{r}
# Model Accuracy
accuracy_class_tree <- sum(diag(table_class_tree)) / sum(table_class_tree)
accuracy_class_tree
```

The model accuracy is **84.7%**

### Section 2: Classification Accuracy

We shall use the `caret` function `confusionMatrix` to calculate model metrics of accuracy, sensitivity, specificity, PPV, NPV, etc and compute ROC and AUC

```{r}
# Logistic regression model
summ_log <- confusionMatrix(table_log, positive="Yes")
# Discriminant analysis model
summ_lda <- confusionMatrix(table_lda, positive="Yes")
# Classification tree
summ_class_tree <- confusionMatrix(table_class_tree, positive="Yes")
```

Combine all metrics to one table
```{r}
# retrieve details from all model objects and combine

all_mod <- cbind(Model = c("Logistic","LDA",
                           "Classification tree"),
                 rbind(summ_log$overall, summ_lda$overall,
                 summ_class_tree$overall))

# Make data frame and kable it
all_mod |> as.data.frame() |> kable(digits = 2)

```
In terms of accuracy, the linear discriminant analysis model was the best model.

Further, we compute and plot ROC and AUC

**Logistic regression model**
```{r, fig.height=8, fig.width=8, fig.cap="ROC Curve of Logistic Regression Model"}
# Calculate prediction object
pred_log <-prediction(predictions = predicted_log, labels = test.data$AHD)
# Compute ROC
roc_log <- performance(pred_log, measure ="tpr", x.measure ="fpr")
# Plot the ROC
plot(roc_log, main="ROC curve: Logistic regression model", col ="blue",
     lwd = 3)
segments(0, 0, 1, 1, lty = 3)
```

**Linear Discriminant Analysis Model**
```{r, fig.height=8, fig.width=8, fig.cap="ROC Curve of Linear Discriminant Analysis Model"}
# Create prediction object
pred_lda <- prediction(predictions = unlist(predictions.lda$posterior)[,2],
                       labels = test.data$AHD)
# Compute ROC
roc_lda<-performance(pred_lda, measure = "tpr", x.measure = "fpr")
# Plot ROC
plot(roc_lda, main = "ROC curve: LDA  model", col ="blue", lwd = 3)
segments(0, 0, 1, 1, lty = 3)

```


**Classification tree model**
```{r, fig.height=8, fig.width=8, fig.cap="ROC Curve of Classification Tree Model"}
# Create prediction object
pred_cla <- prediction(predictions = unlist(pred_prob)[,2],
                       labels = test.data$AHD)
# Compute ROC
roc_cla<-performance(pred_cla, measure="tpr", x.measure="fpr")
# Plot curve
plot(roc_cla, main="ROC curve: Classification Tree  Model",
     col = "blue", lwd = 3)
segments(0, 0, 1, 1, lty=3)

```

## Part b

We build regression models using mutliple regression and regression trees from the *Boston* data using *caret* and *5-fold cross validation*

We start by fitting a multiple linear model with our target variable `medv` which is the median value of owner-occupied homes in $1000's
```{r}
# Multiple linear regression model and 5 fold c.v
set.seed(123)
fitControl <- trainControl(method = "cv",   
                           number = 5)     # number of folds
  
model_lm <- train(medv ~ .,
               data = Boston,
               method = "lm",
               trControl = fitControl)
model_lm
```

Our RMSE is **4.81**. The Rsquared is **0.73** and the MAE is **3.34**.

Next, we model a regression tree with the same data.We split the data into training and testing sets.

```{r}
set.seed(123)
# create partition
samples_boston <- Boston$medv %>% 
  createDataPartition(p = 0.8, list = FALSE)
# Assign train and test
bost_train  <- Boston[samples_boston, ]
bost_test <- Boston[-samples_boston, ]
```

Fitting the model:

```{r}
bost_1 <- rpart(
  formula = medv ~ .,
  data    = bost_train,
  method  = "anova"
  )
# Glimpse model
bost_1
```

We start with 407 observations at the root node (very beginning) and the first variable we split on (the first variable that optimizes a reduction in SSE) is `crim`. 

```{r,fig.height=8, fig.width=8, fig.cap="Regresion tree of the Boston housing data"}
# Plot regression tree
rpart.plot(bost_1)
```


We then check the most important variables.

```{r}
# View most important variables
bost_1$variable.importance
```
The most important variables are average number of rooms per dwelling and % lower status of the population.

If we apply the model on the test data:

```{r}
# Predict
pred_regt <- predict(bost_1, bost_test)
RMSE(pred_regt, bost_test$medv)
```
The RMSE of the regression tree is **4.93**.

This means that the regression model performed better than the 5-fold multiple regression model.

# Appendix

## Heart Data

**Attribute Information:**

>+ _Age_: Age
>+ _Sex_: Sex (1 = male; 0 = female)
>+ _ChestPain_: Chest pain (typical, asymptotic, nonanginal, nontypical)
>+ _RestBP_: Resting blood pressure
>+ _Chol_: Serum cholestoral in mg/dl
>+ _Fbs_: Fasting blood sugar > 120 mg/dl (1 = true; 0 = false)
>+ _RestECG_: Resting electrocardiographic results
>+ _MaxHR_: Maximum heart rate achieved
>+ _ExAng:_ Exercise induced angina (1 = yes; 0 = no)
>+ _Oldpeak_: ST depression induced by exercise relative to rest
>+ _Slope_: Slope of the peak exercise ST segment
>+ _Ca_: Number of major vessels colored by flourosopy (0 - 3)
>+ _Thal_: (3 = normal; 6 = fixed defect; 7 = reversable defect)
>+ _target: AHD_ - Diagnosis of heart disease (1 = yes; 0 = no)

Source: https://archive.ics.uci.edu/ml/datasets/Heart+Disease

**Data Set Information:

This database contains attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. 

## Boston Data

**Attribute Information:**

>+ _crim_ - per capita crime rate by town
>+ _zn_ - proportion of residential land zoned for lots over 25,000 sq.ft.
>+ _indus_ - proportion of non-retail business acres per town.
>+ _chas_ - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
>+ _nox_ - nitric oxides concentration (parts per 10 million)
>+ _rm _ - average number of rooms per dwelling
>+ _age_ - proportion of owner-occupied units built prior to 1940
>+ _dis_ - weighted distances to five Boston employment centres
>+ _rad_ - index of accessibility to radial highways
>+ _tax_ - full-value property-tax rate per $10,000
>+ _ptratio_ - pupil-teacher ratio by town
>+ _black_ - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
>+ _lstat_ - % lower status of the population
>+ _medv_ - Median value of owner-occupied homes in $1000's